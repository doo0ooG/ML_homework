# 步骤一：数据预处理

#### 任务描述

在`./data/json`目录下有三个文件，分别对应训练，验证，测试集，但是为了后续训练分词器，先把三个数据集合并

`./data/get_corpus.py`负责将数据分成中英文后重新打包得到`corpus.zh`和`corpus.en`

#### 代码更改

- 首先，使用更加现代的`pathlib`库实现对文件的读写操作
- 使用异常处理使程序更有健壮性





# 步骤二：分词器训练

#### 功能描述

使用`sentencePiece`分别训练了中英文的分词器模型

依托步骤一生成的`corpus.zh`和`corpus.en`，使用脚本`./data/tokenizer/tokenize.py`训练分词器

#### 代码更改

- 改了一个老掉牙的字符串拼接，改为更加现代的`f-string`





# 步骤三：构造数据集和训练和验证

## 1. 首先实现一些功能函数和配置文件

`./utils.py`的函数`set_logger(log_path)`， 设置全局logger将训练信息输出到日志文件和控制台



## 2. 其次定义函数`data_loader.py`实现数据集的构造

- 构造`MTDataset`类
  - 成员函数`get_dataset`和`len_argsort`用于将句子按照长度sort，**这样一个`batch`内句子的长度接近**，`padding`少，**有助于提升模型性能**！
  - Batch的构造，将mask改为bool，用来适应**Pytorch写法的Transformer**


# 步骤四：定义模型

- 改为Pytorch自带的Transformer
- 权重初始化


# 步骤五：训练

- 梯度裁剪
- AMP
- 预热，衰退
- 标签平滑
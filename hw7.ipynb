{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "400ab11e",
   "metadata": {},
   "source": [
    "# 1.导包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82544599",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from accelerate import Accelerator\n",
    "from transformers import BertTokenizerFast, BertForQuestionAnswering\n",
    "from torch.optim import AdamW\n",
    "import os\n",
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import get_cosine_schedule_with_warmup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319730b9",
   "metadata": {},
   "source": [
    "# 2. 固定随机种子，选择设备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03fb17b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 选择设备\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# 固定随机种子\n",
    "def same_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "same_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "834db788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 选择是否开启自动混合精度\n",
    "amp_fp16 = True\n",
    "\n",
    "if amp_fp16:\n",
    "    accelerator = Accelerator(mixed_precision = 'fp16')\n",
    "    device = accelerator.device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c545898",
   "metadata": {},
   "source": [
    "# 3.加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae8d9d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model exist.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'bert-base-chinese'\n",
    "model_saved_path = './bert-use-chinese'\n",
    "\n",
    "if os.path.exists(model_saved_path + '/vocab.txt') == False:\n",
    "    print(\"downloading model......\")\n",
    "    os.mkdir(model_saved_path)\n",
    "    tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
    "    tokenizer.save_pretrained(model_saved_path)\n",
    "    model = BertForQuestionAnswering.from_pretrained(model_name).to(device)\n",
    "    model.save_pretrained(model_saved_path)\n",
    "else:\n",
    "    print(\"model exist.\")\n",
    "    tokenizer = BertTokenizerFast.from_pretrained(model_saved_path)\n",
    "    model = BertForQuestionAnswering.from_pretrained(model_saved_path).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a4f01c",
   "metadata": {},
   "source": [
    "# 4.加载数据和数据集构造"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a9eaf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "def read_data(file):\n",
    "    with open(file, \"r\", encoding = 'utf-8') as reader:\n",
    "        data = json.load(reader)\n",
    "    return data['questions'], data['paragraphs']\n",
    "\n",
    "train_questions, train_paragraphs = read_data('./data/hw7_train.json')\n",
    "dev_questions, dev_paragraphs = read_data('./data/hw7_dev.json')\n",
    "test_questions, test_paragraphs = read_data('./data/hw7_test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0b8bc5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (570 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "train_questions_tokenized = tokenizer([train_question[\"question_text\"] for train_question in train_questions], add_special_tokens=False)\n",
    "dev_questions_tokenized = tokenizer([dev_question[\"question_text\"] for dev_question in dev_questions], add_special_tokens=False)\n",
    "test_questions_tokenized = tokenizer([test_question[\"question_text\"] for test_question in test_questions], add_special_tokens=False) \n",
    "\n",
    "train_paragraphs_tokenized = tokenizer(train_paragraphs, add_special_tokens=False)\n",
    "dev_paragraphs_tokenized = tokenizer(dev_paragraphs, add_special_tokens=False)\n",
    "test_paragraphs_tokenized = tokenizer(test_paragraphs, add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f3e8b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义数据集\n",
    "class QA_Dataset(Dataset):\n",
    "    def __init__(self, split, questions, tokenized_questions, tokenized_paragraphs):\n",
    "        self.split = split\n",
    "        self.questions = questions\n",
    "        self.tokenized_questions = tokenized_questions\n",
    "        self.tokenized_paragraphs = tokenized_paragraphs\n",
    "        self.max_question_len = 64\n",
    "        self.max_paragraph_len = 256\n",
    "\n",
    "        #####  TODO: Change doc_strides  #####\n",
    "        self.doc_stride = 128\n",
    "\n",
    "        self.max_seq_len = 1 + self.max_question_len + 1 + self.max_paragraph_len + 1\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "    \n",
    "    def get_random_window_start(self, start, end):\n",
    "        start_max = start\n",
    "        start_min = max(0, end - self.max_paragraph_len + 1)\n",
    "        if start_min > start_max:\n",
    "            window_start = start_min\n",
    "        else:\n",
    "            window_start = np.random.randint(start_min, start_max + 1)\n",
    "        return window_start\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        question = self.questions[idx]\n",
    "        tokenized_question = self.tokenized_questions[idx]\n",
    "        tokenized_paragraph = self.tokenized_paragraphs[question[\"paragraph_id\"]]\n",
    "\n",
    "        # ##### TODO: How to prevent model from learning somthing it should not learn #####\n",
    "        if self.split == \"train\":\n",
    "            answer_start_token = tokenized_paragraph.char_to_token(question['answer_start'])\n",
    "            answer_end_token = tokenized_paragraph.char_to_token(question['answer_end'])\n",
    "            window_start = self.get_random_window_start(answer_start_token, answer_end_token)\n",
    "            window_end = window_start + self.max_paragraph_len\n",
    "\n",
    "            input_ids_question = [101] + tokenized_question.ids[:self.max_question_len] + [102]\n",
    "            input_ids_paragraph = tokenized_paragraph.ids[window_start:window_end] + [102]\n",
    "\n",
    "            # Convert answer's start/end positions in tokenized_paragraph to start/end positions in the window  \n",
    "            answer_start_token += len(input_ids_question) - window_start\n",
    "            answer_end_token += len(input_ids_question) - window_start\n",
    "            \n",
    "            # Pad sequence and obtain inputs to model \n",
    "            input_ids, token_type_ids, attention_mask = self.padding(input_ids_question, input_ids_paragraph)\n",
    "            return torch.tensor(input_ids), torch.tensor(token_type_ids), torch.tensor(attention_mask), answer_start_token, answer_end_token\n",
    "\n",
    "        # Validation/Testing\n",
    "        else:\n",
    "            input_ids_list, token_type_ids_list, attention_mask_list = [], [], []\n",
    "            \n",
    "            # Paragraph is split into several windows, each with start positions separated by step \"doc_stride\"\n",
    "            for i in range(0, len(tokenized_paragraph), self.doc_stride):\n",
    "                \n",
    "                # Slice question/paragraph and add special tokens (101: CLS, 102: SEP)\n",
    "                input_ids_question = [101] + tokenized_question.ids[:self.max_question_len] + [102]\n",
    "                input_ids_paragraph = tokenized_paragraph.ids[i : i + self.max_paragraph_len] + [102]\n",
    "                \n",
    "                # Pad sequence and obtain inputs to model\n",
    "                input_ids, token_type_ids, attention_mask = self.padding(input_ids_question, input_ids_paragraph)\n",
    "                \n",
    "                input_ids_list.append(input_ids)\n",
    "                token_type_ids_list.append(token_type_ids)\n",
    "                attention_mask_list.append(attention_mask)\n",
    "            \n",
    "            return torch.tensor(input_ids_list), torch.tensor(token_type_ids_list), torch.tensor(attention_mask_list)\n",
    "\n",
    "    def padding(self, input_ids_question, input_ids_paragraph):\n",
    "        padding_len = self.max_seq_len - len(input_ids_question) - len(input_ids_paragraph)\n",
    "        assert padding_len >= 0, f\"padding_len negative! qlen={len(input_ids_question)}, plen={len(input_ids_paragraph)}, max={self.max_seq_len}\"\n",
    "        input_ids = input_ids_question + input_ids_paragraph + [0] * padding_len\n",
    "        token_type_ids = [0] * len(input_ids_question) + [1] * len(input_ids_paragraph) + [0] * padding_len\n",
    "        attention_mask = [1] * (len(input_ids_question) + len(input_ids_paragraph)) + [0] * padding_len\n",
    "        # 确保每个list长度都等于self.max_seq_len\n",
    "        assert len(input_ids) == self.max_seq_len, f\"input_ids: {len(input_ids)} != {self.max_seq_len}\"\n",
    "        assert len(token_type_ids) == self.max_seq_len, f\"token_type_ids: {len(token_type_ids)} != {self.max_seq_len}\"\n",
    "        assert len(attention_mask) == self.max_seq_len, f\"attention_mask: {len(attention_mask)} != {self.max_seq_len}\"\n",
    "        return input_ids, token_type_ids, attention_mask\n",
    "\n",
    "\n",
    "train_set = QA_Dataset(\"train\", train_questions, train_questions_tokenized, train_paragraphs_tokenized)\n",
    "dev_set = QA_Dataset(\"dev\", dev_questions, dev_questions_tokenized, dev_paragraphs_tokenized)\n",
    "test_set = QA_Dataset(\"test\", test_questions, test_questions_tokenized, test_paragraphs_tokenized)\n",
    "\n",
    "train_batch_size = 32\n",
    "\n",
    "# Note: Do NOT change batch size of dev_loader / test_loader !\n",
    "# Although batch size=1, it is actually a batch consisting of several windows from the same QA pair\n",
    "train_loader = DataLoader(train_set, batch_size=train_batch_size, shuffle=True, pin_memory=True)\n",
    "dev_loader = DataLoader(dev_set, batch_size=1, shuffle=False, pin_memory=True)\n",
    "test_loader = DataLoader(test_set, batch_size=1, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08d424e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data, output):\n",
    "    ##### TODO: Postprocessing #####\n",
    "    # There is a bug and room for improvement in postprocessing \n",
    "    # Hint: Open your prediction file to see what is wrong \n",
    "    \n",
    "    answer = ''\n",
    "    max_prob = float('-inf')\n",
    "    num_of_windows = data[0].shape[1]\n",
    "    \n",
    "    for k in range(num_of_windows):\n",
    "        # Obtain answer by choosing the most probable start position / end position\n",
    "        start_prob, start_index = torch.max(output.start_logits[k], dim=0)\n",
    "        end_prob, end_index = torch.max(output.end_logits[k], dim=0)\n",
    "        \n",
    "        # Probability of answer is calculated as sum of start_prob and end_prob\n",
    "        prob = start_prob + end_prob\n",
    "        \n",
    "        # Replace answer if calculated probability is larger than previous windows\n",
    "        if prob > max_prob:\n",
    "            max_prob = prob\n",
    "            # Convert tokens to chars (e.g. [1920, 7032] --> \"大 金\")\n",
    "            answer = tokenizer.decode(data[0][0][k][start_index : end_index + 1])\n",
    "    \n",
    "    # Remove spaces in answer (e.g. \"大 金\" --> \"大金\")\n",
    "    return answer.replace(' ','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "759ac7fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 99/991 [00:32<05:05,  2.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Step 100 | loss = 4.734, acc = 0.030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 199/991 [01:05<04:37,  2.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Step 200 | loss = 1.895, acc = 0.371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 299/991 [01:38<03:54,  2.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Step 300 | loss = 1.097, acc = 0.597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 399/991 [02:10<03:22,  2.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Step 400 | loss = 0.967, acc = 0.634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 499/991 [02:43<02:46,  2.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Step 500 | loss = 0.879, acc = 0.661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 599/991 [03:15<02:17,  2.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Step 600 | loss = 0.845, acc = 0.665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 699/991 [03:48<01:39,  2.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Step 700 | loss = 0.821, acc = 0.680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 799/991 [04:20<01:04,  3.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Step 800 | loss = 0.797, acc = 0.677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 899/991 [04:53<00:30,  2.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Step 900 | loss = 0.799, acc = 0.698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 991/991 [05:22<00:00,  3.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Dev Set ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4131/4131 [00:59<00:00, 69.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation | Epoch 1 | acc = 0.664\n",
      "Saving Model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 99/991 [00:32<05:00,  2.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Step 100 | loss = 0.718, acc = 0.699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 199/991 [01:04<04:25,  2.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Step 200 | loss = 0.692, acc = 0.718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 299/991 [01:37<03:53,  2.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Step 300 | loss = 0.671, acc = 0.726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 399/991 [02:09<03:18,  2.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Step 400 | loss = 0.691, acc = 0.720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 499/991 [02:41<02:44,  3.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Step 500 | loss = 0.669, acc = 0.728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 599/991 [03:13<02:10,  3.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Step 600 | loss = 0.692, acc = 0.717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 699/991 [03:46<01:38,  2.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Step 700 | loss = 0.687, acc = 0.724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 799/991 [04:18<01:04,  2.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Step 800 | loss = 0.689, acc = 0.725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 899/991 [04:50<00:31,  2.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Step 900 | loss = 0.703, acc = 0.723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 991/991 [05:20<00:00,  3.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Dev Set ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4131/4131 [00:58<00:00, 70.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation | Epoch 2 | acc = 0.696\n",
      "Saving Model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 99/991 [00:32<05:04,  2.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Step 100 | loss = 0.586, acc = 0.755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 199/991 [01:05<04:29,  2.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Step 200 | loss = 1.065, acc = 0.646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 299/991 [01:37<03:57,  2.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Step 300 | loss = 0.791, acc = 0.692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 399/991 [02:10<03:22,  2.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Step 400 | loss = 0.656, acc = 0.732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 499/991 [02:43<02:49,  2.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Step 500 | loss = 0.590, acc = 0.749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 599/991 [03:15<02:10,  3.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Step 600 | loss = 0.532, acc = 0.772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 699/991 [03:48<01:37,  2.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Step 700 | loss = 0.545, acc = 0.776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 799/991 [04:20<01:05,  2.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Step 800 | loss = 0.536, acc = 0.775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 899/991 [04:53<00:31,  2.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Step 900 | loss = 0.509, acc = 0.783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 991/991 [05:22<00:00,  3.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Dev Set ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4131/4131 [01:01<00:00, 66.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation | Epoch 3 | acc = 0.682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 99/991 [00:31<05:01,  2.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Step 100 | loss = 0.448, acc = 0.798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 168/991 [00:54<04:26,  3.09it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 48\u001b[0m\n\u001b[0;32m     43\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m     46\u001b[0m accelerator\u001b[38;5;241m.\u001b[39mbackward(output\u001b[38;5;241m.\u001b[39mloss)\n\u001b[1;32m---> 48\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep() \n\u001b[0;32m     50\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\Doog\\.conda\\envs\\doog\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:140\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m opt \u001b[38;5;241m=\u001b[39m opt_ref()\n\u001b[0;32m    139\u001b[0m opt\u001b[38;5;241m.\u001b[39m_opt_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m--> 140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(opt, opt\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Doog\\.conda\\envs\\doog\\lib\\site-packages\\accelerate\\optimizer.py:166\u001b[0m, in \u001b[0;36mAcceleratedOptimizer.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_patched_step_method\n\u001b[1;32m--> 166\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accelerate_step_called:\n\u001b[0;32m    170\u001b[0m         \u001b[38;5;66;03m# If the optimizer step was skipped, gradient overflow was detected.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Doog\\.conda\\envs\\doog\\lib\\site-packages\\torch\\amp\\grad_scaler.py:457\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[1;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    455\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 457\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_opt_step(optimizer, optimizer_state, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    459\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mSTEPPED\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[1;32mc:\\Users\\Doog\\.conda\\envs\\doog\\lib\\site-packages\\torch\\amp\\grad_scaler.py:351\u001b[0m, in \u001b[0;36mGradScaler._maybe_opt_step\u001b[1;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    345\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    349\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[0;32m    350\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 351\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf_per_device\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    352\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[1;32mc:\\Users\\Doog\\.conda\\envs\\doog\\lib\\site-packages\\torch\\amp\\grad_scaler.py:351\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    345\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    349\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[0;32m    350\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 351\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m    352\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epoch = 10\n",
    "validation = True\n",
    "logging_step = 100\n",
    "learning_rate = 1e-4\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "best_val_acc = 0\n",
    "\n",
    "model, optimizer, train_loader = accelerator.prepare(model, optimizer, train_loader) \n",
    "\n",
    "model.train()\n",
    "\n",
    "\n",
    "num_training_steps = num_epoch * len(train_loader)\n",
    "num_warmup_steps = int(0.1 * num_training_steps)  # 前10%用于warmup\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Start Training ...\")\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    step = 1\n",
    "    train_loss = train_acc = 0\n",
    "    \n",
    "    for data in tqdm(train_loader):\t\n",
    "        # Load all data into GPU\n",
    "        data = [i.to(device) for i in data]\n",
    "        \n",
    "        # Model inputs: input_ids, token_type_ids, attention_mask, start_positions, end_positions (Note: only \"input_ids\" is mandatory)\n",
    "        # Model outputs: start_logits, end_logits, loss (return when start_positions/end_positions are provided)  \n",
    "        output = model(input_ids=data[0], token_type_ids=data[1], attention_mask=data[2], start_positions=data[3], end_positions=data[4])\n",
    "\n",
    "        # Choose the most probable start position / end position\n",
    "        start_index = torch.argmax(output.start_logits, dim=1)\n",
    "        end_index = torch.argmax(output.end_logits, dim=1)\n",
    "        \n",
    "        # Prediction is correct only if both start_index and end_index are correct\n",
    "        train_acc += ((start_index == data[3]) & (end_index == data[4])).float().mean()\n",
    "        train_loss += output.loss\n",
    "        \n",
    "        \n",
    "        accelerator.backward(output.loss)\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step() \n",
    "        optimizer.zero_grad()\n",
    "        step += 1\n",
    "\n",
    "        ##### TODO: Apply linear learning rate decay #####\n",
    "        \n",
    "        \n",
    "        # Print training loss and accuracy over past logging step\n",
    "        if step % logging_step == 0:\n",
    "            print(f\"Epoch {epoch + 1} | Step {step} | loss = {train_loss.item() / logging_step:.3f}, acc = {train_acc / logging_step:.3f}\")\n",
    "            train_loss = train_acc = 0\n",
    "\n",
    "    if validation:\n",
    "        print(\"Evaluating Dev Set ...\")\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            dev_acc = 0\n",
    "            for i, data in enumerate(tqdm(dev_loader)):\n",
    "                output = model(input_ids=data[0].squeeze(dim=0).to(device), token_type_ids=data[1].squeeze(dim=0).to(device),\n",
    "                       attention_mask=data[2].squeeze(dim=0).to(device))\n",
    "                # prediction is correct only if answer text exactly matches\n",
    "                dev_acc += evaluate(data, output) == dev_questions[i][\"answer_text\"]\n",
    "            print(f\"Validation | Epoch {epoch + 1} | acc = {dev_acc / len(dev_loader):.3f}\")\n",
    "            if dev_acc > best_val_acc:\n",
    "                best_val_acc = dev_acc\n",
    "                print(\"Saving Model ...\")\n",
    "                model_save_dir = \"saved_model\" \n",
    "                model.save_pretrained(model_save_dir)\n",
    "        model.train()\n",
    "\n",
    "# Save a model and its configuration file to the directory 「saved_model」 \n",
    "# i.e. there are two files under the direcory 「saved_model」: 「pytorch_model.bin」 and 「config.json」\n",
    "# Saved model can be re-loaded using 「model = BertForQuestionAnswering.from_pretrained(\"saved_model\")」\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f795ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Test Set ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4957/4957 [01:13<00:00, 67.77it/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating Test Set ...\")\n",
    "\n",
    "result = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(test_loader):\n",
    "        output = model(input_ids=data[0].squeeze(dim=0).to(device), token_type_ids=data[1].squeeze(dim=0).to(device),\n",
    "                       attention_mask=data[2].squeeze(dim=0).to(device))\n",
    "        result.append(evaluate(data, output))\n",
    "\n",
    "result_file = \"with_learningratedecay_change_stride_improve_pre_epoch_amp.csv\"\n",
    "\n",
    "with open(result_file, 'w') as f:\n",
    "    f.write('ID,Answer\\n')\n",
    "    for i, test_question in enumerate(test_questions):\n",
    "        f.write(f\"{test_question['id'], {result[i].replace(',', '')}}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8685b093",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "input_file = \"with_learningratedecay_change_stride_improve_pre_epoch_amp.csv\"\n",
    "output_file = \"res_1.csv\"\n",
    "\n",
    "with open(input_file, encoding=\"utf-8\") as fin, open(output_file, \"w\", encoding=\"utf-8\") as fout:\n",
    "    fout.write(\"ID,Answer\\n\")\n",
    "    for line in fin:\n",
    "        # 跳过表头\n",
    "        if line.strip().startswith(\"ID,\"):\n",
    "            continue\n",
    "        # 正则提取ID和答案内容\n",
    "        match = re.match(r\"\\((\\d+), \\{[\\\"']?(.*?)[\\\"']?\\}\\)\", line.strip())\n",
    "        if match:\n",
    "            idx, answer = match.groups()\n",
    "            fout.write(f\"{idx},{answer}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a53041f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "doog",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

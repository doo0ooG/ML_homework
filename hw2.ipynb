{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b44386f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_feat(path):\n",
    "    return torch.load(path)\n",
    "\n",
    "def shift(x, n):\n",
    "    if n < 0:\n",
    "        left = x[0].repeat(-n, 1)\n",
    "        right = x[:n]\n",
    "    elif n > 0:\n",
    "        right = x[-1].repeat(n, 1)\n",
    "        left = x[n:]\n",
    "    else:\n",
    "        return x\n",
    "    return torch.cat((left, right), dim = 0)\n",
    "\n",
    "def concat_feat(x, concat_n):\n",
    "    assert concat_n % 2 == 1\n",
    "    if concat_n == 1:\n",
    "        return x\n",
    "    \n",
    "    seq_len, feature_len = x.size(0), x.size(1)\n",
    "    x = x.repeat(1, concat_n)\n",
    "    x = x.view(seq_len, concat_n, feature_len).permute(1, 0, 2)\n",
    "    mid = (concat_n) // 2\n",
    "    for r_idx in range(1, mid + 1):\n",
    "        x[mid + r_idx, :] = shift(x[mid + r_idx], r_idx)\n",
    "        x[mid - r_idx, :] = shift(x[mid - r_idx], -r_idx)\n",
    "    return x.permute(1, 0, 2).view(seq_len, concat_n * feature_len)\n",
    "\n",
    "def preprocess_data(split, feat_dir, phone_path, concat_nframes, train_ratio = 0.8, train_val_seed = 1337):\n",
    "    class_num = 41\n",
    "    mode = 'train' if (split == 'train' or split == 'val') else 'test'\n",
    "\n",
    "    label_dict = {}\n",
    "\n",
    "    if mode != 'test':\n",
    "        phone_file = open(os.path.join(phone_path, f'{mode}_labels.txt')).readlines()\n",
    "\n",
    "        for line in phone_file:\n",
    "            line = line.strip('\\n').split(' ')\n",
    "            label_dict[line[0]] = [int(p) for p in line[1:]]\n",
    "\n",
    "    if split == 'train' or split == 'val':\n",
    "        usage_list = open(os.path.join(phone_path, 'train_split.txt')).readlines()\n",
    "        random.seed(train_val_seed)\n",
    "        random.shuffle(usage_list)\n",
    "        percent = int(len(usage_list) * train_ratio)\n",
    "        usage_list = usage_list[:percent] if split == 'train' else usage_list[percent:]\n",
    "    elif split == 'test':\n",
    "        usage_list = open(os.path.join(phone_path, 'test_split.txt')).readlines()\n",
    "    else:\n",
    "        raise ValueError(\"Invalid 'split' argument for dataset: PhoneDataset!\")\n",
    "    \n",
    "    usage_list = [id.strip('\\n') for id in usage_list]\n",
    "    print('[Dataset] - # Phone classes: ' + str(class_num) + ', number of utterancees for ' + split + ': ' + str(len(usage_list)))\n",
    "\n",
    "    max_len = 3000000\n",
    "    X = torch.empty(max_len, 39 * concat_nframes)\n",
    "    if mode != 'test':\n",
    "        y = torch.empty(max_len, dtype=torch.long)\n",
    "    \n",
    "    idx = 0\n",
    "    for i, frame in tqdm(enumerate(usage_list)):\n",
    "        feat = load_feat(os.path.join(feat_dir, mode, f'{frame}.pt'))\n",
    "        cur_len = feat.size(0)\n",
    "        feat = concat_feat(feat, concat_nframes)\n",
    "        X[idx:idx + cur_len] = feat\n",
    "        if mode != 'test':\n",
    "            y[idx:idx + cur_len] = torch.LongTensor(label_dict[frame])\n",
    "        idx += cur_len\n",
    "\n",
    "    X = X[:idx, :]\n",
    "    if mode != 'test':\n",
    "        y = y[:idx]\n",
    "\n",
    "    print(f'[INFO] {split} set')\n",
    "    print(X.shape)\n",
    "    if mode != 'test':\n",
    "        print(y.shape)\n",
    "        return X, y\n",
    "    else:\n",
    "        return X\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d61a28a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class LibriDataset(Dataset):\n",
    "    def __init__(self, X, y = None):\n",
    "        self.data = X\n",
    "        if y is not None:\n",
    "            self.label = torch.LongTensor(y)\n",
    "        else:\n",
    "            self.label = None\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.label is None:\n",
    "            return self.data[idx]\n",
    "        else:\n",
    "            return self.data[idx], self.label[idx]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452daa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BasicalBlock(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(BasicalBlock, self).__init__()\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Linear(input_dim, output_dim),\n",
    "            nn.BatchNorm1d(output_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        ) \n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "    \n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim = 41, hidden_layers = 1, hidden_dim = 256):\n",
    "        super(Classifier, self).__init__()\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            BasicalBlock(input_dim, hidden_dim),\n",
    "            *[BasicalBlock(hidden_dim, hidden_dim) for _ in range(hidden_layers)],\n",
    "            BasicalBlock(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "335cebe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_nframes = 7\n",
    "train_ratio = 0.8\n",
    "seed = 0\n",
    "batch_size = 512\n",
    "num_epoch = 20\n",
    "learning_rate = 0.0001\n",
    "model_path = './model.ckpt'\n",
    "input_dim = 39 * concat_nframes\n",
    "hidden_layers = 6\n",
    "hidden_dim = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "598337d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] - # Phone classes: 41, number of utterancees for train: 3428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]C:\\Users\\Doog\\AppData\\Local\\Temp\\ipykernel_25408\\3476177765.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(path)\n",
      "3428it [00:03, 1119.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] train set\n",
      "torch.Size([2121270, 273])\n",
      "torch.Size([2121270])\n",
      "[Dataset] - # Phone classes: 41, number of utterancees for val: 858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "858it [00:00, 1090.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] val set\n",
      "torch.Size([522888, 273])\n",
      "torch.Size([522888])\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "train_X, train_y = preprocess_data('train', './libriphone/feat', './libriphone', concat_nframes, train_ratio, seed)\n",
    "valid_X, valid_y = preprocess_data('val', './libriphone/feat', './libriphone', concat_nframes, train_ratio, seed)\n",
    "\n",
    "train_set = LibriDataset(train_X, train_y)\n",
    "valid_set = LibriDataset(valid_X, valid_y)\n",
    "\n",
    "# del train_X, train_y, valid_X, valid_y\n",
    "# gc.collect()\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size = batch_size, shuffle = True)\n",
    "valid_loader = DataLoader(valid_set, batch_size = batch_size, shuffle = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dc410a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEVICE]: cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'[DEVICE]: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f02057e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d7589c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def same_seed(seed):\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2c00f259",
   "metadata": {},
   "outputs": [],
   "source": [
    "same_seed(seed)\n",
    "model = Classifier(input_dim = input_dim, hidden_layers = hidden_layers, hidden_dim = hidden_dim).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d9309730",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4144/4144 [00:33<00:00, 122.19it/s]\n",
      "100%|██████████| 1022/1022 [00:04<00:00, 224.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[001/020] Train Acc: 0.369456 Loss: 2.609383 | Valid Acc: 0.533967 loss: 1.886129\n",
      "saving model with acc 0.534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4144/4144 [00:32<00:00, 126.12it/s]\n",
      "100%|██████████| 1022/1022 [00:04<00:00, 222.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[002/020] Train Acc: 0.440023 Loss: 2.260134 | Valid Acc: 0.566500 loss: 1.687021\n",
      "saving model with acc 0.566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4144/4144 [00:32<00:00, 126.08it/s]\n",
      "100%|██████████| 1022/1022 [00:04<00:00, 222.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[003/020] Train Acc: 0.457830 Loss: 2.167588 | Valid Acc: 0.582209 loss: 1.593015\n",
      "saving model with acc 0.582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4144/4144 [00:32<00:00, 128.38it/s]\n",
      "100%|██████████| 1022/1022 [00:04<00:00, 208.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[004/020] Train Acc: 0.468056 Loss: 2.122470 | Valid Acc: 0.591964 loss: 1.550244\n",
      "saving model with acc 0.592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4144/4144 [00:33<00:00, 125.38it/s]\n",
      "100%|██████████| 1022/1022 [00:04<00:00, 220.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[005/020] Train Acc: 0.474968 Loss: 2.096812 | Valid Acc: 0.598933 loss: 1.523008\n",
      "saving model with acc 0.599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4144/4144 [00:32<00:00, 128.67it/s]\n",
      "100%|██████████| 1022/1022 [00:04<00:00, 229.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[006/020] Train Acc: 0.479999 Loss: 2.078324 | Valid Acc: 0.603376 loss: 1.503840\n",
      "saving model with acc 0.603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4144/4144 [00:31<00:00, 129.87it/s]\n",
      "100%|██████████| 1022/1022 [00:04<00:00, 231.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[007/020] Train Acc: 0.484796 Loss: 2.061013 | Valid Acc: 0.608911 loss: 1.486745\n",
      "saving model with acc 0.609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4144/4144 [00:31<00:00, 130.98it/s]\n",
      "100%|██████████| 1022/1022 [00:04<00:00, 228.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[008/020] Train Acc: 0.487873 Loss: 2.051089 | Valid Acc: 0.612936 loss: 1.471270\n",
      "saving model with acc 0.613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4144/4144 [00:31<00:00, 130.92it/s]\n",
      "100%|██████████| 1022/1022 [00:04<00:00, 226.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[009/020] Train Acc: 0.491194 Loss: 2.037631 | Valid Acc: 0.615625 loss: 1.458977\n",
      "saving model with acc 0.616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4144/4144 [00:31<00:00, 130.53it/s]\n",
      "100%|██████████| 1022/1022 [00:04<00:00, 229.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[010/020] Train Acc: 0.494187 Loss: 2.028665 | Valid Acc: 0.617786 loss: 1.452220\n",
      "saving model with acc 0.618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4144/4144 [00:31<00:00, 129.66it/s]\n",
      "100%|██████████| 1022/1022 [00:04<00:00, 230.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[011/020] Train Acc: 0.496038 Loss: 2.021993 | Valid Acc: 0.619307 loss: 1.447081\n",
      "saving model with acc 0.619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4144/4144 [00:32<00:00, 129.20it/s]\n",
      "100%|██████████| 1022/1022 [00:04<00:00, 227.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[012/020] Train Acc: 0.498333 Loss: 2.014434 | Valid Acc: 0.622858 loss: 1.433493\n",
      "saving model with acc 0.623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4144/4144 [00:32<00:00, 128.94it/s]\n",
      "100%|██████████| 1022/1022 [00:03<00:00, 261.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[013/020] Train Acc: 0.500564 Loss: 2.008138 | Valid Acc: 0.624128 loss: 1.432170\n",
      "saving model with acc 0.624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4144/4144 [00:31<00:00, 129.63it/s]\n",
      "100%|██████████| 1022/1022 [00:03<00:00, 263.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[014/020] Train Acc: 0.501993 Loss: 2.002454 | Valid Acc: 0.625721 loss: 1.426740\n",
      "saving model with acc 0.626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4144/4144 [00:32<00:00, 127.61it/s]\n",
      "100%|██████████| 1022/1022 [00:04<00:00, 226.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[015/020] Train Acc: 0.503641 Loss: 1.996609 | Valid Acc: 0.626404 loss: 1.421590\n",
      "saving model with acc 0.626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4144/4144 [00:31<00:00, 130.35it/s]\n",
      "100%|██████████| 1022/1022 [00:04<00:00, 233.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[016/020] Train Acc: 0.505971 Loss: 1.990244 | Valid Acc: 0.629089 loss: 1.412805\n",
      "saving model with acc 0.629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4144/4144 [00:32<00:00, 128.79it/s]\n",
      "100%|██████████| 1022/1022 [00:04<00:00, 234.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[017/020] Train Acc: 0.506431 Loss: 1.987105 | Valid Acc: 0.629473 loss: 1.407950\n",
      "saving model with acc 0.629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4144/4144 [00:34<00:00, 119.13it/s]\n",
      "100%|██████████| 1022/1022 [00:05<00:00, 204.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[018/020] Train Acc: 0.507742 Loss: 1.984467 | Valid Acc: 0.630204 loss: 1.408531\n",
      "saving model with acc 0.630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4144/4144 [00:34<00:00, 121.47it/s]\n",
      "100%|██████████| 1022/1022 [00:04<00:00, 211.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[019/020] Train Acc: 0.508663 Loss: 1.979960 | Valid Acc: 0.631464 loss: 1.402508\n",
      "saving model with acc 0.631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4144/4144 [00:33<00:00, 125.57it/s]\n",
      "100%|██████████| 1022/1022 [00:04<00:00, 212.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[020/020] Train Acc: 0.509753 Loss: 1.977051 | Valid Acc: 0.632330 loss: 1.401185\n",
      "saving model with acc 0.632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "best_acc = 0.0\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    train_acc = 0.0\n",
    "    train_loss = 0.0\n",
    "    valid_acc = 0.0\n",
    "    valid_loss = 0.0\n",
    "\n",
    "    model.train()\n",
    "    for i, batch in enumerate(tqdm(train_loader)):\n",
    "        features, labels = batch\n",
    "        features = features.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(features)\n",
    "\n",
    "        loss = criterion(preds, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        _, train_pred = torch.max(preds, 1)\n",
    "        train_acc += (train_pred.detach() == labels.detach()).sum().item()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    if len(valid_set) > 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, batch in enumerate(tqdm(valid_loader)):\n",
    "                features, labels = batch\n",
    "                features = features.to(device)\n",
    "                labels = labels.to(device)\n",
    "                preds = model(features)\n",
    "\n",
    "                loss = criterion(preds, labels)\n",
    "\n",
    "                _, valid_pred = torch.max(preds, 1)\n",
    "                valid_acc += (valid_pred.cpu() == labels.cpu()).sum().item()\n",
    "                valid_loss += loss.item()\n",
    "\n",
    "            print('[{:03d}/{:03d}] Train Acc: {:3.6f} Loss: {:3.6f} | Valid Acc: {:3.6f} loss: {:3.6f}'.format(\n",
    "                epoch + 1, num_epoch, train_acc/len(train_set), train_loss/len(train_loader), valid_acc/len(valid_set), valid_loss/len(valid_loader)\n",
    "            ))\n",
    "\n",
    "            if valid_acc > best_acc:\n",
    "                best_acc = valid_acc\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "                print('saving model with acc {:.3f}'.format(best_acc/len(valid_set)))\n",
    "    else:\n",
    "        print('[{:03d}/{:03d}] Train Acc: {:3.6f} Loss: {:3.6f}'.format(\n",
    "            epoch + 1, num_epoch, train_acc/len(train_set), train_loss/len(train_loader)\n",
    "        ))\n",
    "\n",
    "if len(valid_set) == 0:\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print('saving model at last epoch') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cb4374ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] - # Phone classes: 41, number of utterancees for test: 1078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]C:\\Users\\Doog\\AppData\\Local\\Temp\\ipykernel_25408\\3229498859.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(path)\n",
      "1078it [00:00, 1256.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] test set\n",
      "torch.Size([646268, 273])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 载入数据\n",
    "test_X = preprocess_data(split='test', feat_dir='./libriphone/feat', phone_path='./libriphone', concat_nframes=concat_nframes)\n",
    "test_set = LibriDataset(test_X, None)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bbd6766d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1263/1263 [00:03<00:00, 351.25it/s]\n"
     ]
    }
   ],
   "source": [
    "test_acc = 0.0\n",
    "test_lengths = 0\n",
    "pred = np.array([], dtype=np.int32)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(tqdm(test_loader)):\n",
    "        features = batch\n",
    "        features = features.to(device)\n",
    "\n",
    "        outputs = model(features)\n",
    "\n",
    "        _, test_pred = torch.max(outputs, 1) # 获得概率最高的类的索引\n",
    "        pred = np.concatenate((pred, test_pred.cpu().numpy()), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0acf1a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('prediction.csv', 'w') as f:\n",
    "    f.write('Id,Class\\n')\n",
    "    for i, y in enumerate(pred):\n",
    "        f.write('{},{}\\n'.format(i, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e588e277",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
